import logging
import hashlib
from typing import List, Dict, Any, Optional
from pathlib import Path
from src.domain.ports.embedding import EmbeddingConnector
from src.domain.ports.vector import VectorConnector, Document, SearchResult
from src.tools.text_extract import PDFChapterExtractor


class VectorService:
    """Service qui orchestre la vectorisation et la recherche de documents."""

    def __init__(
            self,
            vector_adapter: VectorConnector,
            embedding_adapter: EmbeddingConnector,
    ):
        self.vector_adapter = vector_adapter
        self.embedding_adapter = embedding_adapter
        self.logger = logging.getLogger(__name__)

    def initialize(self, vector_config: Dict[str, Any], embedding_config: Dict[str, Any]) -> None:
        """
        Initialise les deux adaptateurs.

        Args:
            vector_config: Configuration pour la base vectorielle
            embedding_config: Configuration pour le g√©n√©rateur d'embeddings
        """
        self.logger.info("üöÄ Initialisation du VectorService")

        try:
            self.embedding_adapter.initialize(embedding_config)
            self.vector_adapter.initialize(vector_config)

            if not self.vector_adapter.health_check():
                raise Exception("Health check de la base vectorielle a √©chou√©")

            self.logger.info("‚úÖ VectorService initialis√© avec succ√®s")

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'initialisation: {e}")
            raise

    def add_pdf_document_by_chapters(
            self,
            pdf_path: Path,
            pdf_name: Optional[str] = None
    ) -> None:
        """
        Ajoute un document PDF √† la base vectorielle en le d√©coupant par chapitres.

        Args:
            pdf_path: Chemin vers le fichier PDF
            pdf_name: Nom du fichier PDF (optionnel, utilise le nom du fichier si non fourni)
        """
        if pdf_name is None:
            pdf_name = pdf_path.name

        self.logger.info(f"üìÑ Ajout du document par chapitres: {pdf_name}")

        try:
            # G√©n√©ration d'un ID unique pour le document
            document_id = self._generate_document_id(pdf_name, pdf_path)

            # V√©rification si le document existe d√©j√†
            if self.vector_adapter.document_exists(document_id):
                self.logger.info(f"üìã Document {pdf_name} d√©j√† pr√©sent, suppression de l'ancien")
                self.remove_pdf_document(pdf_name, pdf_path)

            # Extraction par chapitres
            with PDFChapterExtractor(str(pdf_path)) as extractor:
                chapters = extractor.extract_chapters()

            self.logger.info(f"üìö Document d√©coup√© en {len(chapters)} chapitres")

            # Cr√©ation des documents avec m√©tadonn√©es
            documents = []
            for i, chapter in enumerate(chapters):
                chapter_id = f"{document_id}_chapter_{i}"

                # Pr√©paration du contenu avec titre int√©gr√©
                content = f"# {chapter['title']}\n\n{chapter['content']}"

                doc = Document(
                    id=chapter_id,
                    content=content,
                    metadata={
                        "source_pdf": pdf_name,
                        "pdf_path": str(pdf_path),
                        "chapter_index": i,
                        "chapter_title": chapter['title'],
                        "total_chapters": len(chapters),
                        "original_document_id": document_id,
                        "content_type": "chapter",
                        "content_length": len(chapter['content'])
                    }
                )
                documents.append(doc)

            # G√©n√©ration des embeddings par batch pour optimiser
            self._generate_embeddings_for_documents(documents)

            # Ajout √† la base vectorielle
            self.vector_adapter.add_documents(documents)

            self.logger.info(f"‚úÖ Document {pdf_name} ajout√© avec {len(chapters)} chapitres")

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'ajout du document {pdf_name}: {e}")
            raise

    def add_pdf_document(
            self,
            pdf_name: str,
            text: str,
            pdf_path: Path,
            chunk_size: int = 1000,
            chunk_overlap: int = 200
    ) -> None:
        """
        Ajoute un document PDF √† la base vectorielle en le d√©coupant en chunks.
        (M√©thode originale conserv√©e pour compatibilit√©)

        Args:
            pdf_name: Nom du fichier PDF
            text: Contenu textuel du PDF
            pdf_path: Chemin vers le fichier PDF
            chunk_size: Taille des chunks en caract√®res
            chunk_overlap: Chevauchement entre chunks
        """
        self.logger.info(f"üìÑ Ajout du document par chunks: {pdf_name}")

        try:
            # G√©n√©ration d'un ID unique pour le document
            document_id = self._generate_document_id(pdf_name, pdf_path)

            # V√©rification si le document existe d√©j√†
            if self.vector_adapter.document_exists(document_id):
                self.logger.info(f"üìã Document {pdf_name} d√©j√† pr√©sent, suppression de l'ancien")
                self.vector_adapter.delete_documents([document_id])

            # D√©coupage du texte en chunks
            chunks = self._chunk_text(text, chunk_size, chunk_overlap)
            self.logger.info(f"‚úÇÔ∏è Texte d√©coup√© en {len(chunks)} chunks")

            # Cr√©ation des documents avec m√©tadonn√©es
            documents = []
            for i, chunk in enumerate(chunks):
                chunk_id = f"{document_id}_chunk_{i}"
                doc = Document(
                    id=chunk_id,
                    content=chunk,
                    metadata={
                        "source_pdf": pdf_name,
                        "pdf_path": str(pdf_path),
                        "chunk_index": i,
                        "total_chunks": len(chunks),
                        "original_document_id": document_id,
                        "content_type": "chunk"
                    }
                )
                documents.append(doc)

            # G√©n√©ration des embeddings par batch pour optimiser
            self._generate_embeddings_for_documents(documents)

            # Ajout √† la base vectorielle
            self.vector_adapter.add_documents(documents)

            self.logger.info(f"‚úÖ Document {pdf_name} ajout√© avec {len(chunks)} chunks")

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'ajout du document {pdf_name}: {e}")
            raise

    def search_documents(
            self,
            query: str,
            top_k: int = 5,
            filters: Optional[Dict[str, Any]] = None
    ) -> List[SearchResult]:
        """
        Recherche des documents similaires √† la requ√™te.

        Args:
            query: Requ√™te de recherche
            top_k: Nombre de r√©sultats √† retourner
            filters: Filtres optionnels sur les m√©tadonn√©es

        Returns:
            Liste des r√©sultats de recherche
        """
        self.logger.info(f"üîç Recherche: '{query}' (top_{top_k})")

        try:
            results = self.vector_adapter.search_similar(query, top_k, filters)
            self.logger.info(f"üìä {len(results)} r√©sultats trouv√©s")
            return results

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la recherche: {e}")
            raise

    def search_by_pdf(self, query: str, pdf_name: str, top_k: int = 5) -> List[SearchResult]:
        """
        Recherche dans un PDF sp√©cifique.

        Args:
            query: Requ√™te de recherche
            pdf_name: Nom du PDF √† cibler
            top_k: Nombre de r√©sultats

        Returns:
            Liste des r√©sultats de recherche
        """
        filters = {"source_pdf": pdf_name}
        return self.search_documents(query, top_k, filters)

    def search_by_chapter(self, query: str, pdf_name: str, top_k: int = 5) -> List[SearchResult]:
        """
        Recherche dans les chapitres d'un PDF sp√©cifique.

        Args:
            query: Requ√™te de recherche
            pdf_name: Nom du PDF √† cibler
            top_k: Nombre de r√©sultats

        Returns:
            Liste des r√©sultats de recherche
        """
        filters = {"source_pdf": pdf_name, "content_type": "chapter"}
        return self.search_documents(query, top_k, filters)

    def get_context_for_query(
            self,
            query: str,
            max_context_length: int = 4000,
            top_k: int = 10,
            prefer_chapters: bool = True
    ) -> str:
        """
        R√©cup√®re le contexte le plus pertinent pour une requ√™te.
        Utile pour alimenter un LLM avec du contexte.

        Args:
            query: Requ√™te
            max_context_length: Longueur maximale du contexte
            top_k: Nombre de chunks √† consid√©rer
            prefer_chapters: Privil√©gier les chapitres plut√¥t que les chunks

        Returns:
            Contexte format√©
        """
        # Filtrer par type de contenu si demand√©
        filters = {"content_type": "chapter"} if prefer_chapters else None
        results = self.search_documents(query, top_k, filters)

        # Si pas assez de r√©sultats avec chapitres, rechercher dans tout
        if prefer_chapters and len(results) < 3:
            results = self.search_documents(query, top_k)

        context_parts = []
        current_length = 0

        for result in results:
            chunk_text = result.document.content
            source = result.document.metadata.get("source_pdf", "Unknown")

            # Ajouter titre du chapitre si disponible
            chapter_title = result.document.metadata.get("chapter_title")
            if chapter_title:
                formatted_chunk = f"[{source} - {chapter_title}]\n{chunk_text}\n"
            else:
                formatted_chunk = f"[{source}]\n{chunk_text}\n"

            if current_length + len(formatted_chunk) > max_context_length:
                break

            context_parts.append(formatted_chunk)
            current_length += len(formatted_chunk)

        return "\n---\n".join(context_parts)

    def remove_pdf_document(self, pdf_name: str, pdf_path: Path) -> None:
        """
        Supprime tous les chunks/chapitres d'un document PDF.

        Args:
            pdf_name: Nom du PDF
            pdf_path: Chemin du PDF
        """
        self.logger.info(f"üóëÔ∏è Suppression du document: {pdf_name}")

        try:
            document_id = self._generate_document_id(pdf_name, pdf_path)

            # Recherche de tous les chunks/chapitres du document
            filters = {"original_document_id": document_id}
            results = self.vector_adapter.search_similar("", top_k=1000, filters=filters)

            chunk_ids = [result.document.id for result in results]

            if chunk_ids:
                self.vector_adapter.delete_documents(chunk_ids)
                self.logger.info(f"‚úÖ {len(chunk_ids)} √©l√©ments supprim√©s pour {pdf_name}")
            else:
                self.logger.info(f"üìã Aucun √©l√©ment trouv√© pour {pdf_name}")

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la suppression de {pdf_name}: {e}")
            raise

    def get_statistics(self) -> Dict[str, Any]:
        """
        Retourne des statistiques sur la base vectorielle.

        Returns:
            Dictionnaire avec les statistiques
        """
        try:
            total_docs = self.vector_adapter.get_document_count()
            embedding_dim = self.embedding_adapter.get_dimension()


            return {
                "total_documents": total_docs,
                "embedding_dimension": embedding_dim,
                "health_status": self.vector_adapter.health_check()
            }

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la r√©cup√©ration des statistiques: {e}")
            return {"error": str(e)}

    def _generate_document_id(self, pdf_name: str, pdf_path: Path) -> str:
        """G√©n√®re un ID unique pour un document bas√© sur son nom et chemin."""
        content = f"{pdf_name}_{pdf_path}"
        return hashlib.md5(content.encode()).hexdigest()

    def _chunk_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """
        D√©coupe un texte en chunks avec chevauchement.

        Args:
            text: Texte √† d√©couper
            chunk_size: Taille des chunks
            overlap: Chevauchement

        Returns:
            Liste des chunks
        """
        if len(text) <= chunk_size:
            return [text]

        chunks = []
        start = 0

        while start < len(text):
            end = start + chunk_size

            # Si on n'est pas √† la fin, essaie de couper √† un espace
            if end < len(text):
                # Cherche le dernier espace dans les 100 derniers caract√®res
                last_space = text.rfind(' ', end - 100, end)
                if last_space > start:
                    end = last_space

            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)

            # Avance avec chevauchement
            start = end - overlap

            # √âvite les boucles infinies
            if start >= end:
                start = end

        return chunks

    def _generate_embeddings_for_documents(self, documents: List[Document]) -> None:
        """
        G√©n√®re les embeddings pour une liste de documents.

        Args:
            documents: Liste des documents
        """
        texts = [doc.content for doc in documents]

        try:
            # G√©n√©ration par batch pour optimiser
            embeddings = self.embedding_adapter.generate_embeddings_batch(texts)

            for doc, embedding in zip(documents, embeddings):
                doc.embedding = embedding

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de la g√©n√©ration des embeddings: {e}")
            # Fallback: g√©n√©ration individuelle
            for doc in documents:
                try:
                    doc.embedding = self.embedding_adapter.generate_embedding(doc.content)
                except Exception as embedding_error:
                    self.logger.error(f"‚ùå Erreur embedding pour chunk: {embedding_error}")
                    raise